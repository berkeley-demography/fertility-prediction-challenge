---
title: "Logistic model"
format: html
editor: source
---

```{r}
library(here)
library(tidyverse)
library(tidymodels)
library(bundle)
```

```{r}
raw_data_dir <- here("data")
prepped_data_dir <- here("data-prepped")
```

## Open up the data

```{r}
rec <- unbundle(readRDS(file=here(prepped_data_dir, 'recipe.rds')))
                
train_data <- readRDS(file=here(prepped_data_dir, 'train_split.rds'))

df_train <- train_data$df_train
folds <- train_data$folds
```


BOOSTED TREES (ENSEMBLE OF DECISION TREES)

TODO - tune the number of trees, other params

```{r, eval=FALSE}
use_xgboost(new_child ~ ., data=df_train)
```


```{r}
boosted_trees_model <-
  #boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
  #    loss_reduction = tune(), sample_size = tune()) %>% 
  boost_tree(trees = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("xgboost")   
  #boost_tree(trees=20) %>%
  #set_engine('xgboost') %>%
  #set_mode('classification')

boosted_trees_workflow <-
  workflow() %>%
  add_model(boosted_trees_model) %>%
  add_recipe(rec)

set.seed(52097)

boosted_trees_tuned <-
  tune_grid(boosted_trees_workflow, 
            resamples = folds,
            metrics = metric_set(recall, precision, f_meas))
            # let's see how auto grid works
            #grid = stop("add number of candidate points"))

boosted_trees_best_mod <- boosted_trees_tuned %>% 
  select_best(metric = "f_meas")

boosted_trees_fit <-
  finalize_workflow(boosted_trees_workflow, boosted_trees_best_mod) %>%
  fit(data = df_train)
```


```{r}
saveRDS(bundle(boosted_trees_fit),
        file=here(prepped_data_dir, 'fit_boosted_trees.rds'))
```

